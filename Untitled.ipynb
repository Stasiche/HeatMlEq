{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-dbda5e54a3fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mReshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.core import Flatten, Dropout\n",
    "from keras.layers import Input, merge\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import matplotlib.pyplot as plt\n",
    "from misc_layers import MinibatchDiscrimination, SubPixelUpscaling, CustomLRELU, bilinear2x\n",
    "from keras_contrib.layers import SubPixelUpscaling\n",
    "import keras.backend as K\n",
    "from keras.initializers import RandomNormal\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "import glob\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "from sklearn.utils import shuffle\n",
    "import scipy\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "import keras.backend as K\n",
    "from scipy.interpolate import spline\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def get_gen_normal(noise_shape):\n",
    "    noise_shape = noise_shape\n",
    "    \n",
    "    kernel_init = 'glorot_uniform'\n",
    "    \n",
    "    gen_input = Input(shape = noise_shape)\n",
    "    generator = Conv2DTranspose(filters = 512, kernel_size = (4,4), strides = (1,1), padding = \"valid\", data_format = \"channels_last\", kernel_initializer = kernel_init)(gen_input)\n",
    "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
    "    generator = LeakyReLU(0.2)(generator)\n",
    "        \n",
    "    generator = Conv2DTranspose(filters = 256, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
    "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
    "    generator = LeakyReLU(0.2)(generator)\n",
    "    \n",
    "    generator = Conv2DTranspose(filters = 128, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
    "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
    "    generator = LeakyReLU(0.2)(generator)\n",
    "    \n",
    "    generator = Conv2DTranspose(filters = 64, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
    "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
    "    generator = LeakyReLU(0.2)(generator)\n",
    "    \n",
    "    generator = Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
    "    generator = BatchNormalization(momentum = 0.5)(generator)\n",
    "    generator = LeakyReLU(0.2)(generator)\n",
    "    \n",
    "    generator = Conv2DTranspose(filters = 3, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(generator)\n",
    "    generator = Activation('tanh')(generator)\n",
    "        \n",
    "    gen_opt = Adam(lr=0.00015, beta_1=0.5)\n",
    "    generator_model = Model(input = gen_input, output = generator)\n",
    "    generator_model.compile(loss='binary_crossentropy', optimizer=gen_opt, metrics=['accuracy'])\n",
    "    generator_model.summary()\n",
    "\n",
    "    return generator_model\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def get_disc_normal(image_shape=(64,64,3)):\n",
    "    image_shape = image_shape\n",
    "    \n",
    "    dropout_prob = 0.4\n",
    "    kernel_init = 'glorot_uniform'\n",
    "    dis_input = Input(shape = image_shape)\n",
    "    discriminator = Conv2D(filters = 64, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(dis_input)\n",
    "    discriminator = LeakyReLU(0.2)(discriminator)\n",
    "    discriminator = Conv2D(filters = 128, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(discriminator)\n",
    "    discriminator = BatchNormalization(momentum = 0.5)(discriminator)\n",
    "    discriminator = LeakyReLU(0.2)(discriminator)\n",
    "   \n",
    "    discriminator = Conv2D(filters = 256, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(discriminator)\n",
    "    discriminator = BatchNormalization(momentum = 0.5)(discriminator)\n",
    "    discriminator = LeakyReLU(0.2)(discriminator)\n",
    "    \n",
    "    discriminator = Conv2D(filters = 512, kernel_size = (4,4), strides = (2,2), padding = \"same\", data_format = \"channels_last\", kernel_initializer = kernel_init)(discriminator)\n",
    "    discriminator = BatchNormalization(momentum = 0.5)(discriminator)\n",
    "    discriminator = LeakyReLU(0.2)(discriminator)\n",
    "    \n",
    "    discriminator = Flatten()(discriminator)\n",
    "    \n",
    "    discriminator = Dense(1)(discriminator)\n",
    "    discriminator = Activation('sigmoid')(discriminator)\n",
    "    #also try the SGD optimiser, might work better for a few learning rates.\n",
    "    dis_opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    discriminator_model = Model(input = dis_input, output = discriminator)\n",
    "    discriminator_model.compile(loss='binary_crossentropy', optimizer=dis_opt, metrics=['accuracy'])\n",
    "    discriminator_model.summary()\n",
    "    return discriminator_model\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def norm_img(img):\n",
    "    img = (img / 127.5) - 1\n",
    "    #image normalisation to keep values between -1 and 1 for stability\n",
    "    return img\n",
    "\n",
    "def denorm_img(img):\n",
    "    #for output\n",
    "    img = (img + 1) * 127.5\n",
    "    return img.astype(np.uint8) \n",
    "\n",
    "\n",
    "def sample_from_dataset(batch_size, image_shape, data_dir=None, data = None):\n",
    "    sample_dim = (batch_size,) + image_shape\n",
    "    sample = np.empty(sample_dim, dtype=np.float32)\n",
    "    all_data_dirlist = list(glob.glob(data_dir))\n",
    "    sample_imgs_paths = np.random.choice(all_data_dirlist,batch_size)\n",
    "    for index,img_filename in enumerate(sample_imgs_paths):\n",
    "        image = Image.open(img_filename)\n",
    "        image = image.resize(image_shape[:-1])\n",
    "        image = image.convert('RGB')\n",
    "        image = np.asarray(image)\n",
    "        image = norm_img(image)\n",
    "        sample[index,...] = image\n",
    "    return sample\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def gen_noise(batch_size, noise_shape):\n",
    "    #input noise for the generator should follow a probability distribution, like in this case, the normal distributon.\n",
    "    return np.random.normal(0, 1, size=(batch_size,)+noise_shape)\n",
    "\n",
    "\n",
    "def generate_images(generator, save_dir):\n",
    "    \n",
    "    noise = gen_noise(batch_size,noise_shape)\n",
    "    fake_data_X = generator.predict(noise)\n",
    "    print(\"Displaying generated images\")\n",
    "    plt.figure(figsize=(4,4))\n",
    "    gs1 = gridspec.GridSpec(4, 4)\n",
    "    gs1.update(wspace=0, hspace=0)\n",
    "    rand_indices = np.random.choice(fake_data_X.shape[0],16,replace=False)\n",
    "    for i in range(16):\n",
    "        #plt.subplot(4, 4, i+1)\n",
    "        ax1 = plt.subplot(gs1[i])\n",
    "        ax1.set_aspect('equal')\n",
    "        rand_index = rand_indices[i]\n",
    "        image = fake_data_X[rand_index, :,:,:]\n",
    "        fig = plt.imshow(denorm_img(image))\n",
    "        plt.axis('off')\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir+str(time.time())+\"_GENimage.png\",bbox_inches='tight',pad_inches=0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def save_img_batch(img_batch,img_save_dir):\n",
    "    plt.figure(figsize=(4,4))\n",
    "    gs1 = gridspec.GridSpec(4, 4)\n",
    "    gs1.update(wspace=0, hspace=0)\n",
    "    rand_indices = np.random.choice(img_batch.shape[0],16,replace=False)\n",
    "    for i in range(16):\n",
    "        #plt.subplot(4, 4, i+1)\n",
    "        ax1 = plt.subplot(gs1[i])\n",
    "        ax1.set_aspect('equal')\n",
    "        rand_index = rand_indices[i]\n",
    "        image = img_batch[rand_index, :,:,:]\n",
    "        fig = plt.imshow(denorm_img(image))\n",
    "        plt.axis('off')\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(img_save_dir,bbox_inches='tight',pad_inches=0)\n",
    "    plt.show()   \n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "noise_shape = (1,1,100)\n",
    "num_steps = 10000\n",
    "batch_size = 64\n",
    "image_shape = None\n",
    "img_save_dir = \"Z:/anime/output\"\n",
    "save_model = True\n",
    "image_shape = (64,64,3)\n",
    "data_dir =  \"Z:\\\\anime\\\\data\\\\*.png\"\n",
    "\n",
    "log_dir = img_save_dir\n",
    "save_model_dir = img_save_dir\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "discriminator = get_disc_normal(image_shape)\n",
    "generator = get_gen_normal(noise_shape)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "opt = Adam(lr=0.00015, beta_1=0.5) \n",
    "gen_inp = Input(shape=noise_shape)\n",
    "GAN_inp = generator(gen_inp)\n",
    "GAN_opt = discriminator(GAN_inp)\n",
    "gan = Model(input = gen_inp, output = GAN_opt)\n",
    "gan.compile(loss = 'binary_crossentropy', optimizer = opt, metrics=['accuracy'])\n",
    "gan.summary()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "avg_disc_fake_loss = deque([0], maxlen=250)     \n",
    "avg_disc_real_loss = deque([0], maxlen=250)\n",
    "avg_GAN_loss = deque([0], maxlen=250)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "for step in range(num_steps): \n",
    "    tot_step = step\n",
    "    print(\"Begin step: \", tot_step)\n",
    "    step_begin_time = time.time() \n",
    "    \n",
    "    real_data_X = sample_from_dataset(batch_size, image_shape, data_dir = data_dir)\n",
    "\n",
    "    noise = gen_noise(batch_size,noise_shape)\n",
    "    \n",
    "    fake_data_X = generator.predict(noise)\n",
    "    \n",
    "    if (tot_step % 10) == 0:\n",
    "        step_num = str(tot_step).zfill(4)\n",
    "        save_img_batch(fake_data_X,img_save_dir+step_num+\"_image.png\")\n",
    "\n",
    "    #concatenate real and fake data samples    \n",
    "    data_X = np.concatenate([real_data_X,fake_data_X])\n",
    "    #add noise to the label inputs\n",
    "    real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2\n",
    "    \n",
    "    \n",
    "    fake_data_Y = np.random.random_sample(batch_size)*0.2\n",
    "     \n",
    "    data_Y = np.concatenate((real_data_Y,fake_data_Y))\n",
    "        \n",
    "    discriminator.trainable = True\n",
    "    generator.trainable = False\n",
    "    #training the discriminator on real and fake data can be done together, i.e., \n",
    "    #on the data_x and data_y, OR it can be done \n",
    "    #one by one as performed below. This is the safer choice and gives better results \n",
    "    #as compared to combining the real and generated samples.\n",
    "    dis_metrics_real = discriminator.train_on_batch(real_data_X,real_data_Y)  \n",
    "    dis_metrics_fake = discriminator.train_on_batch(fake_data_X,fake_data_Y)   \n",
    "    \n",
    "    print(\"Disc: real loss: %f fake loss: %f\" % (dis_metrics_real[0], dis_metrics_fake[0]))\n",
    "    \n",
    "    \n",
    "    avg_disc_fake_loss.append(dis_metrics_fake[0])\n",
    "    avg_disc_real_loss.append(dis_metrics_real[0])\n",
    "    \n",
    "    generator.trainable = True\n",
    "\n",
    "    GAN_X = gen_noise(batch_size,noise_shape)\n",
    "\n",
    "    GAN_Y = real_data_Y\n",
    "    \n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    gan_metrics = gan.train_on_batch(GAN_X,GAN_Y)\n",
    "    print(\"GAN loss: %f\" % (gan_metrics[0]))\n",
    "    \n",
    "    text_file = open(log_dir+\"\\\\training_log.txt\", \"a\")\n",
    "    text_file.write(\"Step: %d Disc: real loss: %f fake loss: %f GAN loss: %f\\n\" % (tot_step, dis_metrics_real[0],\n",
    "                                                                                   dis_metrics_fake[0],gan_metrics[0]))\n",
    "    text_file.close()\n",
    "    avg_GAN_loss.append(gan_metrics[0])\n",
    "    \n",
    "        \n",
    "    end_time = time.time()\n",
    "    diff_time = int(end_time - step_begin_time)\n",
    "    print(\"Step %d completed. Time took: %s secs.\" % (tot_step, diff_time))\n",
    "    \n",
    "    if ((tot_step+1) % 500) == 0:\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"Average Disc_fake loss: %f\" % (np.mean(avg_disc_fake_loss)))    \n",
    "        print(\"Average Disc_real loss: %f\" % (np.mean(avg_disc_real_loss)))    \n",
    "        print(\"Average GAN loss: %f\" % (np.mean(avg_GAN_loss)))\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        discriminator.trainable = True\n",
    "        generator.trainable = True\n",
    "        generator.save(save_model_dir+str(tot_step)+\"_GENERATOR_weights_and_arch.hdf5\")\n",
    "        discriminator.save(save_model_dir+str(tot_step)+\"_DISCRIMINATOR_weights_and_arch.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/72/b8/2ef7057c956f1062ffab750a90a6bdcd3de127fb696fb64583c2dfe77aab/tensorflow-2.2.0-cp36-cp36m-win_amd64.whl (459.1MB)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in c:\\virtual_env\\main\\lib\\site-packages (from tensorflow) (1.4.1)\n",
      "Collecting astunparse==1.6.3 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\n",
      "Collecting gast==0.3.3 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.8.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/52/a71156b82dbb8a40833b7a571e22c9e65ca4204a56739f97d3eaa25d111e/protobuf-3.11.3-cp36-cp36m-win_amd64.whl (1.1MB)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454kB)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/97/5b/5d962954bdae84c9d5b06978a15049d947a2dad5b02130b3d984d076c0e1/grpcio-1.28.1-cp36-cp36m-win_amd64.whl (2.1MB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\virtual_env\\main\\lib\\site-packages (from tensorflow) (1.18.4)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/63/a5/e6c07b08b934831ccb8c98ee335e66b7761c5754ee3cabfe4c11d0b1af28/opt_einsum-3.2.1-py3-none-any.whl (63kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\virtual_env\\main\\lib\\site-packages (from tensorflow) (1.14.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting wrapt>=1.11.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/82/f7/e43cefbe88c5fd371f4cf0cf5eb3feccd07515af9fd6cf7dbf1d1793a797/wrapt-1.12.1.tar.gz\n",
      "Collecting absl-py>=0.7.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/1a/53/9243c600e047bd4c3df9e69cfabc1e8004a82cac2e0c484580a78a94ba2a/absl-py-0.9.0.tar.gz (104kB)\n",
      "Collecting keras-preprocessing>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "Collecting wheel>=0.26; python_version >= \"3\" (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8c/23/848298cccf8e40f5bbb59009b32848a4c38f4e7f3364297ab3c3e2e2cd14/wheel-0.34.2-py2.py3-none-any.whl\n",
      "Collecting h5py<2.11.0,>=2.10.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/0b/fa/bee65d2dbdbd3611702aafd128139c53c90a1285f169ba5467aab252e27a/h5py-2.10.0-cp36-cp36m-win_amd64.whl (2.4MB)\n",
      "Collecting google-pasta>=0.1.8 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/1d/fd/4f3ca1516cbb3713259ef229abd9314bba0077ef6070285dde0dd1ed21b2/tensorboard-2.2.1-py3-none-any.whl (3.0MB)\n",
      "Requirement already satisfied: setuptools in c:\\virtual_env\\main\\lib\\site-packages (from protobuf>=3.8.0->tensorflow) (40.6.2)\n",
      "Collecting google-auth<2,>=1.6.3 (from tensorboard<2.3.0,>=2.2.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/45/c562c8d2dadc3dc97347d1d7b218ef94be6d851fd6177dad96fdf5e1c74c/google_auth-1.14.2-py2.py3-none-any.whl (89kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.3.0,>=2.2.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/51/cd/a0c1f9e4582ea64dddf76c1b808b318d01e3b858a51c715bffab1016ecc7/tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl (777kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.3.0,>=2.2.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\virtual_env\\main\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.3.0,>=2.2.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a4/63/eaec2bd025ab48c754b55e8819af0f6a69e2b1e187611dd40cbbe101ee7f/Markdown-3.2.2-py3-none-any.whl (88kB)\n",
      "Collecting werkzeug>=0.11.15 (from tensorboard<2.3.0,>=2.2.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298kB)\n",
      "Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/b3/59/524ffb454d05001e2be74c14745b485681c6ed5f2e625f71d135704c0909/cachetools-4.1.0-py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n",
      "Collecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\virtual_env\\main\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\virtual_env\\main\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\virtual_env\\main\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\virtual_env\\main\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.4.5.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorboard 2.2.1 has requirement setuptools>=41.0.0, but you'll have setuptools 40.6.2 which is incompatible.\n",
      "You are using pip version 18.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\virtual_env\\main\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\virtual_env\\main\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
      "Installing collected packages: wheel, astunparse, gast, protobuf, tensorflow-estimator, grpcio, opt-einsum, termcolor, wrapt, absl-py, keras-preprocessing, h5py, google-pasta, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, tensorboard-plugin-wit, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, werkzeug, tensorboard, tensorflow\n",
      "  Running setup.py install for termcolor: started\n",
      "    Running setup.py install for termcolor: finished with status 'done'\n",
      "  Running setup.py install for wrapt: started\n",
      "    Running setup.py install for wrapt: finished with status 'done'\n",
      "  Running setup.py install for absl-py: started\n",
      "    Running setup.py install for absl-py: finished with status 'done'\n",
      "Successfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.1.0 gast-0.3.3 google-auth-1.14.2 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.28.1 h5py-2.10.0 keras-preprocessing-1.1.0 markdown-3.2.2 oauthlib-3.1.0 opt-einsum-3.2.1 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.0 tensorboard-2.2.1 tensorboard-plugin-wit-1.6.0.post3 tensorflow-2.2.0 tensorflow-estimator-2.2.0 termcolor-1.1.0 werkzeug-1.0.1 wheel-0.34.2 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
